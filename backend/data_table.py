"""DataTableç±» - è¡¨æ ¼æ•°æ®ç®¡ç†ç±»

å°è£…è¡¨æ ¼çš„æ•°æ®å¤„ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ç­›é€‰ã€åˆ†é¡µã€æ’åºç­‰æ“ä½œã€‚
åˆå§‹åŒ–æ—¶ä¼ å…¥DataFrameæ ¼å¼çš„æ•°æ®å’Œåˆ—é…ç½®ã€‚
"""

from pydantic import BaseModel
from typing import Optional, List, Dict, Any, Union
import pandas as pd
import re


class ColumnConfig(BaseModel):
    """åˆ—é…ç½®æ¨¡å‹"""
    prop: str  # å­—æ®µå
    label: str  # åˆ—æ ‡é¢˜
    type: str  # æ•°æ®ç±»å‹: 'string', 'number', 'date', 'boolean', 'bytes'
    sortable: Optional[bool] = True  # æ˜¯å¦å¯æ’åº
    filterable: Optional[bool] = True  # æ˜¯å¦å¯ç­›é€‰
    filterType: Optional[str] = 'text'  # ç­›é€‰ç±»å‹: 'text', 'number', 'select', 'multi-select', 'date', 'none'
    minWidth: Optional[int] = 120  # æœ€å°å®½åº¦
    width: Optional[int] = None  # å›ºå®šå®½åº¦
    fixed: Optional[bool | str] = False  # æ˜¯å¦å›ºå®š: 'left', 'right', False
    options: Optional[List[str]] = None  # ä¸‹æ‹‰é€‰é¡¹ï¼ˆç”¨äºselectç±»å‹ï¼‰


# ä½¿ç”¨TYPE_CHECKINGé¿å…å¾ªç¯å¯¼å…¥
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    try:
        from .api import FilterParams, FilterGroup
    except ImportError:
        from backend.api import FilterParams, FilterGroup


class DataTable:
    """è¡¨æ ¼æ•°æ®ç®¡ç†ç±»
    
    å°è£…è¡¨æ ¼çš„æ•°æ®å¤„ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ç­›é€‰ã€åˆ†é¡µã€æ’åºç­‰æ“ä½œã€‚
    åˆå§‹åŒ–æ—¶ä¼ å…¥DataFrameæ ¼å¼çš„æ•°æ®å’Œåˆ—é…ç½®ã€‚
    """
    
    def __init__(self, dataframe: pd.DataFrame, columns_config: List[ColumnConfig]):
        """
        åˆå§‹åŒ–è¡¨æ ¼ç±»
        
        Args:
            dataframe: pandas DataFrameæ ¼å¼çš„æ•°æ®ï¼ˆå¯ä»¥ä¸ºç©ºï¼Œä½†å¿…é¡»æœ‰æ­£ç¡®çš„åˆ—ç»“æ„ï¼‰
            columns_config: åˆ—é…ç½®åˆ—è¡¨ï¼Œå®šä¹‰æ¯åˆ—çš„å±æ€§ï¼ˆå­—æ®µåã€ç±»å‹ã€ç­›é€‰æ–¹å¼ç­‰ï¼‰
        """
        import threading
        self._lock = threading.RLock()
        
        if dataframe is None:
            raise ValueError("DataFrameä¸èƒ½ä¸ºNone")
        if not columns_config:
            raise ValueError("åˆ—é…ç½®ä¸èƒ½ä¸ºç©º")
        
        # å¦‚æœ DataFrame ä¸ºç©ºï¼Œç¡®ä¿å®ƒæœ‰æ­£ç¡®çš„åˆ—ç»“æ„
        if dataframe.empty:
            # ä»åˆ—é…ç½®ä¸­è·å–åˆ—å
            expected_columns = [col.prop for col in columns_config]
            # åˆ›å»ºå…·æœ‰æ­£ç¡®åˆ—ç»“æ„çš„ç©º DataFrame
            self.dataframe = pd.DataFrame(columns=expected_columns)
        else:
            self.dataframe = dataframe.copy()
        
        self.columns_config = columns_config
        # éªŒè¯åˆ—é…ç½®ä¸­çš„å­—æ®µæ˜¯å¦å­˜åœ¨äºDataFrameä¸­
        self._validate_columns()
    
    @property
    def total_count(self) -> int:
        """è·å–æ€»æ•°æ®é‡"""
        return len(self.dataframe)
    
    def _validate_columns(self):
        """éªŒè¯åˆ—é…ç½®ä¸­çš„å­—æ®µæ˜¯å¦å­˜åœ¨äºDataFrameä¸­"""
        df_columns = set(self.dataframe.columns)
        config_props = {col.prop for col in self.columns_config}
        
        missing_in_df = config_props - df_columns
        if missing_in_df:
            raise ValueError(f"åˆ—é…ç½®ä¸­å®šä¹‰çš„å­—æ®µåœ¨DataFrameä¸­ä¸å­˜åœ¨: {missing_in_df}")
    
    def _parse_number_value(self, value: Any) -> Union[int, float, None]:
        """è§£ææ•°å­—å€¼ï¼Œæ”¯æŒ16è¿›åˆ¶å­—ç¬¦ä¸²ï¼ˆå¦‚0x123ï¼‰"""
        if value is None:
            return None
        if isinstance(value, (int, float)):
            return value
        if isinstance(value, str):
            # å°è¯•è§£æ16è¿›åˆ¶å­—ç¬¦ä¸²
            value_stripped = value.strip()
            if value_stripped.startswith('0x') or value_stripped.startswith('0X'):
                try:
                    return int(value_stripped, 16)
                except ValueError:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ä½œä¸ºæ™®é€šæ•°å­—
                    try:
                        return float(value_stripped) if '.' in value_stripped else int(value_stripped)
                    except ValueError:
                        return None
            else:
                # å°è¯•ä½œä¸ºæ™®é€šæ•°å­—è§£æ
                try:
                    return float(value_stripped) if '.' in value_stripped else int(value_stripped)
                except ValueError:
                    return None
        return None
    
    def _build_pandas_filter(self, filters: Optional['FilterParams'] = None, df: Optional[pd.DataFrame] = None) -> pd.Series:
        """å°†ç­›é€‰æ¡ä»¶è½¬æ¢ä¸ºpandaså¸ƒå°”ç´¢å¼•ï¼ˆåŠ¨æ€å¤„ç†ä»»æ„å­—æ®µï¼‰"""
        # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
        try:
            from .api import FilterParams, FilterGroup, NumberFilter  # type: ignore
        except ImportError:
            from backend.api import FilterParams, FilterGroup, NumberFilter  # type: ignore
        
        # ä½¿ç”¨ä¼ å…¥çš„dfæˆ–self.dataframe
        target_df = df if df is not None else self.dataframe
        
        # å¦‚æœ DataFrame ä¸ºç©ºï¼Œè¿”å›ç©ºæ©ç 
        if target_df.empty:
            return pd.Series([], dtype=bool)
        
        if not filters:
            # ä½¿ç”¨ index åˆ›å»ºæ©ç ï¼Œç¡®ä¿ç´¢å¼•ä¸€è‡´
            return pd.Series([True] * len(target_df), index=target_df.index)
        
        # åˆå§‹åŒ–ç­›é€‰æ©ç ï¼Œä½¿ç”¨ DataFrame çš„ç´¢å¼•
        mask = pd.Series([True] * len(target_df), index=target_df.index)
        
        # è·å–ç­›é€‰å‚æ•°å­—å…¸
        filter_dict = filters.model_dump(exclude_none=True) if hasattr(filters, 'model_dump') else filters.dict(exclude_none=True) if hasattr(filters, 'dict') else {}
        
        # éå†æ‰€æœ‰ç­›é€‰å­—æ®µï¼ˆåŒ…æ‹¬åŠ¨æ€å­—æ®µå’Œæ—§å­—æ®µï¼‰
        for field_name, filter_value in filter_dict.items():
            
            # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨äºDataFrameä¸­
            if field_name not in target_df.columns:
                continue
            
            # æŸ¥æ‰¾å¯¹åº”çš„åˆ—é…ç½®
            col_config = next((c for c in self.columns_config if c.prop == field_name), None)
            if not col_config or not col_config.filterable:
                continue
            
            # æ ¹æ®ç­›é€‰ç±»å‹å¤„ç†
            if col_config.filterType == 'number':
                # æ•°å­—ç±»å‹ç­›é€‰
                # å¤„ç† FilterGroup æˆ– NumberFilter å®ä¾‹
                if isinstance(filter_value, FilterGroup):
                    filters_mask = []
                    for num_filter in filter_value.filters:
                        if num_filter.operator and num_filter.value is not None:
                            op = num_filter.operator
                            val = self._parse_number_value(num_filter.value)
                            if val is None:
                                continue
                            if op == '=':
                                filters_mask.append(target_df[field_name] == val)
                            elif op == '>':
                                filters_mask.append(target_df[field_name] > val)
                            elif op == '<':
                                filters_mask.append(target_df[field_name] < val)
                            elif op == '>=':
                                filters_mask.append(target_df[field_name] >= val)
                            elif op == '<=':
                                filters_mask.append(target_df[field_name] <= val)
                    
                    if filters_mask:
                        logic = filter_value.logic or 'AND'
                        if logic.upper() == 'OR':
                            field_mask = filters_mask[0]
                            for m in filters_mask[1:]:
                                field_mask |= m
                        else:
                            field_mask = filters_mask[0]
                            for m in filters_mask[1:]:
                                field_mask &= m
                        mask &= field_mask
                elif isinstance(filter_value, NumberFilter):
                    if filter_value.operator and filter_value.value is not None:
                        op = filter_value.operator
                        val = self._parse_number_value(filter_value.value)
                        if val is not None:
                            if op == '=':
                                mask &= (target_df[field_name] == val)
                            elif op == '>':
                                mask &= (target_df[field_name] > val)
                            elif op == '<':
                                mask &= (target_df[field_name] < val)
                            elif op == '>=':
                                mask &= (target_df[field_name] >= val)
                            elif op == '<=':
                                mask &= (target_df[field_name] <= val)
                # å¤„ç†å­—å…¸æ ¼å¼ï¼ˆä» JSON è§£ææ¥çš„ï¼‰
                elif isinstance(filter_value, dict):
                    if 'filters' in filter_value:
                        # FilterGroupï¼ˆå¤šæ¡ä»¶ï¼‰
                        filter_group = FilterGroup(**filter_value)
                        filters_mask = []
                        for num_filter in filter_group.filters:
                            if num_filter.operator and num_filter.value is not None:
                                op = num_filter.operator
                                val = self._parse_number_value(num_filter.value)
                                if val is None:
                                    continue
                                if op == '=':
                                    filters_mask.append(target_df[field_name] == val)
                                elif op == '>':
                                    filters_mask.append(target_df[field_name] > val)
                                elif op == '<':
                                    filters_mask.append(target_df[field_name] < val)
                                elif op == '>=':
                                    filters_mask.append(target_df[field_name] >= val)
                                elif op == '<=':
                                    filters_mask.append(target_df[field_name] <= val)
                        
                        if filters_mask:
                            logic = filter_group.logic or 'AND'
                            if logic.upper() == 'OR':
                                field_mask = filters_mask[0]
                                for m in filters_mask[1:]:
                                    field_mask |= m
                            else:
                                field_mask = filters_mask[0]
                                for m in filters_mask[1:]:
                                    field_mask &= m
                            mask &= field_mask
                    elif 'operator' in filter_value or 'value' in filter_value:
                        # NumberFilterï¼ˆå•æ¡ä»¶ï¼‰
                        num_filter = NumberFilter(**filter_value)
                        if num_filter.operator and num_filter.value is not None:
                            op = num_filter.operator
                            val = self._parse_number_value(num_filter.value)
                            if val is not None:
                                if op == '=':
                                    mask &= (target_df[field_name] == val)
                                elif op == '>':
                                    mask &= (target_df[field_name] > val)
                                elif op == '<':
                                    mask &= (target_df[field_name] < val)
                                elif op == '>=':
                                    mask &= (target_df[field_name] >= val)
                                elif op == '<=':
                                    mask &= (target_df[field_name] <= val)
            
            elif col_config.filterType == 'text':
                # æ–‡æœ¬ç­›é€‰
                if isinstance(filter_value, str) and filter_value:
                    # å¯¹äºbytesç±»å‹å­—æ®µï¼Œéœ€è¦å…ˆè½¬æ¢ä¸º16è¿›åˆ¶å­—ç¬¦ä¸²å†ç­›é€‰
                    if col_config.type == 'bytes':
                        # ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œè€Œä¸æ˜¯applyï¼ˆæ€§èƒ½æå‡ï¼‰
                        try:
                            # å°è¯•ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²æ“ä½œï¼ˆå¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼‰
                            if target_df[field_name].dtype == 'object':
                                # æ£€æŸ¥ç¬¬ä¸€ä¸ªéç©ºå€¼æ˜¯å¦ä¸ºbytes
                                sample = target_df[field_name].dropna()
                                if len(sample) > 0 and isinstance(sample.iloc[0], bytes):
                                    # ä½¿ç”¨å‘é‡åŒ–æ“ä½œï¼šæ‰¹é‡è½¬æ¢bytesä¸ºhexå­—ç¬¦ä¸²
                                    # æ³¨æ„ï¼špandasçš„applyåœ¨å¤§é‡æ•°æ®æ—¶å¾ˆæ…¢ï¼Œä½†bytesè½¬æ¢æ— æ³•å®Œå…¨å‘é‡åŒ–
                                    # æˆ‘ä»¬ä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹å¼ï¼šåªå¯¹éç©ºå€¼è¿›è¡Œè½¬æ¢
                                    hex_series = target_df[field_name].apply(
                                        lambda val: ' '.join([f'{b:02X}' for b in val]) if isinstance(val, bytes) else str(val)
                                    )
                                    mask &= hex_series.str.contains(filter_value, case=False, na=False)
                                else:
                                    # å·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ç­›é€‰
                                    mask &= target_df[field_name].astype(str).str.contains(filter_value, case=False, na=False)
                            else:
                                # ç›´æ¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç­›é€‰
                                mask &= target_df[field_name].astype(str).str.contains(filter_value, case=False, na=False)
                        except Exception:
                            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•
                            mask &= target_df[field_name].astype(str).str.contains(filter_value, case=False, na=False)
                    else:
                        # æ™®é€šæ–‡æœ¬å­—æ®µï¼šç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²æ“ä½œï¼ˆå·²ä¼˜åŒ–ï¼‰
                        mask &= target_df[field_name].astype(str).str.contains(filter_value, case=False, na=False)
            
            elif col_config.filterType == 'date':
                # æ—¥æœŸç­›é€‰
                if isinstance(filter_value, str) and filter_value:
                    # å¦‚æœæ˜¯tså­—æ®µï¼ˆæ—¶é—´æˆ³ï¼‰ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                    if field_name == 'ts':
                        from datetime import datetime
                        # ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œï¼ˆè™½ç„¶applyä»ç„¶éœ€è¦ï¼Œä½†å°½é‡å‡å°‘è®¡ç®—ï¼‰
                        try:
                            # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ–‡æœ¬åŒ¹é…ï¼ˆæ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼Œå¦‚åªè¾“å…¥æ—¶é—´ï¼‰
                            def timestamp_to_str(ts):
                                """å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç”¨äºåŒ¹é…"""
                                try:
                                    dt = datetime.fromtimestamp(float(ts))
                                    microseconds = int((float(ts) % 1) * 1000000)
                                    return dt.strftime('%Y-%m-%d %H:%M:%S') + f'.{microseconds:06d}'
                                except (ValueError, OSError):
                                    return str(ts)
                            
                            # å°†tsåˆ—è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ–‡æœ¬åŒ¹é…ï¼ˆæ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼‰
                            # æ³¨æ„ï¼šå¯¹äºå¤§é‡æ•°æ®ï¼Œè¿™ä»ç„¶å¯èƒ½è¾ƒæ…¢ï¼Œä½†æä¾›äº†çµæ´»æ€§
                            ts_str_series = target_df[field_name].apply(timestamp_to_str)
                            mask &= ts_str_series.str.contains(filter_value, case=False, na=False)
                        except Exception:
                            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå°è¯•ç›´æ¥å­—ç¬¦ä¸²åŒ¹é…
                            mask &= target_df[field_name].astype(str).str.contains(filter_value, case=False, na=False)
                    else:
                        # æ™®é€šæ—¥æœŸå­—æ®µï¼Œç›´æ¥å­—ç¬¦ä¸²åŒ¹é…
                        mask &= (target_df[field_name].astype(str) == filter_value)
            
            elif col_config.filterType in ['multi-select', 'select']:
                # å¤šé€‰æˆ–å•é€‰ç­›é€‰
                # ç»Ÿä¸€å¤„ç†ï¼šå¦‚æœæ˜¯å•ä¸ªå€¼ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                if isinstance(filter_value, list):
                    filter_list = filter_value
                elif filter_value is not None and filter_value != '':
                    filter_list = [filter_value]
                else:
                    continue
                
                if len(filter_list) > 0:
                    # ç¡®ä¿ DataFrame åˆ—çš„æ•°æ®ç±»å‹åŒ¹é…
                    try:
                        mask &= target_df[field_name].isin(filter_list)
                    except Exception as e:
                        # å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²åå†ç­›é€‰
                        try:
                            mask &= target_df[field_name].astype(str).isin([str(v) for v in filter_list])
                        except Exception as e2:
                            pass
        
        return mask
    
    def get_list(self, 
                 filters: Optional['FilterParams'] = None,
                 page: int = 1,
                 page_size: int = 100,
                 sort_by: Optional[str] = None,
                 sort_order: Optional[str] = None) -> Dict[str, Any]:
        """è·å–æ•°æ®åˆ—è¡¨ï¼ˆæ”¯æŒç­›é€‰ã€åˆ†é¡µã€æ’åºï¼‰
        
        Args:
            filters: ç­›é€‰æ¡ä»¶
            page: é¡µç 
            page_size: æ¯é¡µå¤§å°
            sort_by: æ’åºå­—æ®µ
            sort_order: æ’åºæ–¹å‘ ('ascending' æˆ– 'descending')
        
        Returns:
            åŒ…å«listã€totalã€pageã€pageSizeçš„å­—å…¸
        """
        # ä½¿ç”¨é”ä¿æŠ¤è¯»å–ï¼Œå¹¶åˆ›å»ºdataframeå¿«ç…§ä»¥ç¡®ä¿æ“ä½œçš„ä¸€è‡´æ€§
        with self._lock:
            current_df = self.dataframe
            # å¦‚æœ dataframe æ˜¯ None (è™½ç„¶åˆå§‹åŒ–æ£€æŸ¥è¿‡ï¼Œä½†ä¸ºäº†å®‰å…¨)
            if current_df is None:
                import logging
                logger = logging.getLogger(__name__)
                logger.error("DataFrame æœªåˆå§‹åŒ– or None")
                return {
                    "list": [],
                    "total": 0,
                    "page": page,
                    "pageSize": page_size
                }
        
        # ä»¥ä¸‹æ“ä½œä½¿ç”¨ current_df å¿«ç…§ï¼Œæ— éœ€æŒæœ‰é”ï¼ˆé™¤éæ¶‰åŠåˆ°å…¶ä»–å…±äº«çŠ¶æ€ï¼‰
        # æ³¨æ„ï¼šcurrent_df æ˜¯ä¸€ä¸ªå¼•ç”¨ï¼Œå¦‚æœ add_data æ›¿æ¢äº† self.dataframeï¼Œcurrent_df æŒ‡å‘æ—§å¯¹è±¡ï¼Œè¿™æ˜¯å®‰å…¨çš„ã€‚
        
        if current_df.empty:
            return {
                "list": [],
                "total": 0,
                "page": page,
                "pageSize": page_size
            }
        
        # éªŒè¯ DataFrame çš„å®Œæ•´æ€§ï¼ˆé˜²æ­¢æ•°æ®è¢«æ„å¤–æ¸…ç©ºï¼‰
        dataframe_length = len(current_df)
        
        # è®°å½•å½“å‰é•¿åº¦ï¼ˆç”¨äºä¸‹æ¬¡éªŒè¯ï¼‰- æ³¨æ„ï¼šå†™å…¥ _last_known_length ä¹Ÿåº”è¯¥æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œä½†è¿™é‡Œåªæ˜¯ç”¨äºæ—¥å¿—ï¼Œæš‚ä¸åŠ é”
        if not hasattr(self, '_last_known_length'):
            self._last_known_length = dataframe_length
        
        if dataframe_length == 0 and self._last_known_length > 0:
            import logging
            logger = logging.getLogger(__name__)
            logger.critical(
                f"ğŸš¨ æ£€æµ‹åˆ° DataFrame è¢«æ„å¤–æ¸…ç©ºï¼ä¸Šæ¬¡å·²çŸ¥é•¿åº¦: {self._last_known_length}, "
                f"å½“å‰é•¿åº¦: {dataframe_length}. è¿™ä¼šå¯¼è‡´è¡¨æ ¼æ˜¾ç¤ºä¸ºç©ºï¼"
            )
        elif dataframe_length < self._last_known_length and self._last_known_length > 100:
            # å¦‚æœæ•°æ®é‡çªç„¶å‡å°‘å¾ˆå¤šï¼Œä¹Ÿè®°å½•è­¦å‘Š
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(
                f"âš ï¸ æ•°æ®é‡å¼‚å¸¸å‡å°‘: ä» {self._last_known_length} å‡å°‘åˆ° {dataframe_length}, "
                f"å‡å°‘äº† {self._last_known_length - dataframe_length} è¡Œ"
            )
        
        # æ›´æ–°è®°å½•çš„é•¿åº¦
        if dataframe_length > 0:
            self._last_known_length = dataframe_length
        
        try:
            # è®°å½•å¼€å§‹æ—¶é—´ï¼ˆç”¨äºæ€§èƒ½ç›‘æ§ï¼‰
            import time as time_module
            start_time = time_module.time()
            
            # æ„å»ºç­›é€‰æ¡ä»¶ - ä¼ å…¥ current_df
            mask = self._build_pandas_filter(filters, df=current_df)
            
            # æ£€æŸ¥ mask æ˜¯å¦æœ‰æ•ˆ
            if len(mask) != len(current_df):
                import logging
                logger = logging.getLogger(__name__)
                logger.warning(
                    f"ç­›é€‰æ©ç é•¿åº¦ä¸åŒ¹é…: mask={len(mask)}, dataframe={len(current_df)}. "
                    f"DataFrameç´¢å¼•: {current_df.index.tolist()[:10] if len(current_df) > 0 else 'empty'}, "
                    f"Maskç´¢å¼•: {mask.index.tolist()[:10] if len(mask) > 0 else 'empty'}"
                )
                # é‡æ–°åˆ›å»ºæ­£ç¡®çš„æ©ç ï¼Œä½¿ç”¨ DataFrame çš„ç´¢å¼•
                mask = pd.Series([True] * len(current_df), index=current_df.index)
            
            # ç¡®ä¿ mask çš„ç´¢å¼•ä¸ dataframe çš„ç´¢å¼•åŒ¹é…
            if not mask.index.equals(current_df.index):
                import logging
                logger = logging.getLogger(__name__)
                logger.warning(
                    f"ç­›é€‰æ©ç ç´¢å¼•ä¸åŒ¹é…: é‡æ–°å¯¹é½ç´¢å¼•. "
                    f"DataFrameç´¢å¼•èŒƒå›´: {current_df.index.min() if len(current_df) > 0 else 'N/A'} - "
                    f"{current_df.index.max() if len(current_df) > 0 else 'N/A'}, "
                    f"Maskç´¢å¼•èŒƒå›´: {mask.index.min() if len(mask) > 0 else 'N/A'} - "
                    f"{mask.index.max() if len(mask) > 0 else 'N/A'}"
                )
                # é‡æ–°åˆ›å»ºæ©ç ï¼Œç¡®ä¿ç´¢å¼•åŒ¹é…
                mask = pd.Series([True] * len(current_df), index=current_df.index)
            
            # æ£€æŸ¥ç­›é€‰åçš„æ•°æ®é‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            filtered_count = int(mask.sum()) if hasattr(mask, 'sum') else len(mask[mask])
            import logging
            logger = logging.getLogger(__name__)
            
            # è®°å½•ç­›é€‰ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            filter_info = {}
            if filters:
                try:
                    filter_info = filters.model_dump(exclude_none=True) if hasattr(filters, 'model_dump') else (filters.dict(exclude_none=True) if hasattr(filters, 'dict') else {})
                except:
                    filter_info = str(filters)
            
            # å‡å°‘æ—¥å¿—é¢‘ç‡ï¼Œä»…åœ¨è°ƒè¯•æˆ–å¼‚å¸¸æ—¶è®°å½•
            # logger.info(...)
            
            if filtered_count == 0 and len(current_df) > 0:
                logger.warning(
                    f"âš ï¸ ç­›é€‰åæ•°æ®ä¸ºç©ºï¼åŸå§‹æ•°æ®é‡: {len(current_df)}, "
                    f"ç­›é€‰æ¡ä»¶: {filter_info}. è¿™å¯èƒ½å¯¼è‡´è¡¨æ ¼æ˜¾ç¤ºä¸ºç©ºï¼"
                )
            
            # ä½¿ç”¨è§†å›¾è€Œä¸æ˜¯copyï¼Œæé«˜æ€§èƒ½ï¼ˆåœ¨ç­›é€‰æ—¶ï¼‰
            filtered_df = current_df[mask]
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ’åº
            needs_sort = sort_by and sort_by in filtered_df.columns
            
            # è®¡ç®—æ€»æ•°ï¼ˆåœ¨æ’åºå‰ï¼Œé¿å…ä¸å¿…è¦çš„è®¡ç®—ï¼‰
            total_count = len(filtered_df)
            
            # æ’åºï¼ˆå¦‚æœéœ€è¦ï¼‰
            if needs_sort:
                ascending = sort_order == 'ascending' if sort_order else True
                # æ’åºæ—¶éœ€è¦copyï¼Œå› ä¸ºä¼šä¿®æ”¹æ•°æ®
                filtered_df = filtered_df.sort_values(by=sort_by, ascending=ascending, na_position='last').copy()
            
            # åˆ†é¡µ
            start_index = (page - 1) * page_size
            end_index = start_index + page_size
            
            # ç¡®ä¿ç´¢å¼•èŒƒå›´æœ‰æ•ˆ
            if start_index >= total_count:
                # è¯·æ±‚çš„é¡µé¢è¶…å‡ºèŒƒå›´ï¼Œè¿”å›ç©º DataFrame
                paginated_df = pd.DataFrame(columns=current_df.columns)
            else:
                # ç¡®ä¿ end_index ä¸è¶…è¿‡æ€»æ•°
                end_index = min(end_index, total_count)
                # åªæœ‰åœ¨éœ€è¦æ—¶æ‰copyï¼ˆå¦‚æœå·²ç»copyè¿‡ï¼Œè¿™é‡Œå°±ä¸éœ€è¦å†copyï¼‰
                if needs_sort:
                    paginated_df = filtered_df.iloc[start_index:end_index]
                else:
                    paginated_df = filtered_df.iloc[start_index:end_index].copy()
            
            # è®°å½•æ€§èƒ½ä¿¡æ¯ï¼ˆä»…åœ¨å¤§æ•°æ®é‡æ—¶ï¼‰
            elapsed_time = time_module.time() - start_time
            if elapsed_time > 0.5 or len(current_df) > 5000:
                logger.debug(
                    f"get_list æ€§èƒ½: æ•°æ®é‡={len(current_df)}, ç­›é€‰å={total_count}, "
                    f"åˆ†é¡µ={page}/{page_size}, è€—æ—¶={elapsed_time:.3f}ç§’"
                )
        except Exception as e:
            # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶è¿”å›ç©ºç»“æœ
            import logging
            logger = logging.getLogger(__name__)
            dataframe_length = len(current_df) if current_df is not None else 'N/A'
            logger.error(
                f"get_list å¤„ç†å¤±è´¥: {str(e)}, dataframeé•¿åº¦={dataframe_length}, "
                f"dataframeæ˜¯å¦ä¸ºç©º={current_df.empty if current_df is not None else 'N/A'}, "
                f"ç­›é€‰æ¡ä»¶={filters.model_dump(exclude_none=True) if filters and hasattr(filters, 'model_dump') else (filters.dict(exclude_none=True) if filters and hasattr(filters, 'dict') else {})}",
                exc_info=True
            )
            # å¦‚æœ DataFrame ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºç»“æœ
            if current_df is None or current_df.empty:
                return {
                    "list": [],
                    "total": 0,
                    "page": page,
                    "pageSize": page_size
                }
            # è¿”å›ç©º DataFrameï¼Œä½†ä¿æŒæ­£ç¡®çš„æ€»æ•°ï¼ˆç”¨äºåˆ†é¡µæ˜¾ç¤ºï¼‰
            paginated_df = pd.DataFrame(columns=current_df.columns)
            # å°è¯•è·å–å®é™…æ€»æ•°ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨0
            try:
                total_count = len(current_df)
            except:
                total_count = 0
        
        # å°†DataFrameè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        data_list = paginated_df.to_dict('records')
        
        # å¤„ç†ç‰¹æ®Šç±»å‹å­—æ®µçš„è½¬æ¢
        for record in data_list:
            for key, value in record.items():
                if isinstance(value, bytes):
                    # å°†bytesè½¬æ¢ä¸º16è¿›åˆ¶å­—ç¬¦ä¸²ï¼Œæ¯ä¸ªå­—èŠ‚ä¹‹é—´åŠ ç©ºæ ¼
                    record[key] = ' '.join([f'{b:02X}' for b in value])
                elif key == 'ts' and isinstance(value, (int, float)):
                    # å°†æ—¶é—´æˆ³ï¼ˆfloatï¼Œå¾®ç§’ç²¾åº¦ï¼‰è½¬æ¢ä¸ºæ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²
                    from datetime import datetime
                    try:
                        dt = datetime.fromtimestamp(float(value))
                        # æå–å¾®ç§’éƒ¨åˆ†ï¼ˆæ—¶é—´æˆ³çš„å°æ•°éƒ¨åˆ†è½¬æ¢ä¸ºå¾®ç§’ï¼‰
                        microseconds = int((float(value) % 1) * 1000000)
                        record[key] = dt.strftime('%Y-%m-%d %H:%M:%S') + f'.{microseconds:06d}'
                    except (ValueError, OSError):
                        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸå€¼
                        pass
        
        return {
            "list": data_list,
            "total": total_count,
            "page": page,
            "pageSize": page_size
        }
    
    def get_columns_config(self) -> Dict[str, Any]:
        """è·å–åˆ—é…ç½®ä¿¡æ¯
        
        Returns:
            åŒ…å«columnsé…ç½®çš„å­—å…¸
        """
        # å°†åˆ—é…ç½®è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        columns_list = []
        for col_config in self.columns_config:
            col_dict = {
                "prop": col_config.prop,
                "label": col_config.label,
                "type": col_config.type,
                "sortable": col_config.sortable,
                "filterable": col_config.filterable,
                "filterType": col_config.filterType,
                "minWidth": col_config.minWidth,
                "fixed": col_config.fixed,
            }
            if col_config.width is not None:
                col_dict["width"] = col_config.width
            if col_config.options is not None:
                col_dict["options"] = col_config.options
            columns_list.append(col_dict)
        
        return {
            "columns": columns_list
        }
    
    def get_row_position(self, row_id: Any, filters: Optional['FilterParams'] = None) -> Dict[str, Any]:
        """è·å–è¡Œåœ¨ç­›é€‰ç»“æœä¸­çš„ä½ç½®
        
        Args:
            row_id: è¡Œçš„IDå€¼
            filters: ç­›é€‰æ¡ä»¶
        
        Returns:
            åŒ…å«foundå’Œpositionçš„å­—å…¸
        """
        # ä½¿ç”¨é”ä¿æŠ¤è¯»å–
        with self._lock:
            current_df = self.dataframe
            if current_df is None:
                return {"found": False, "position": -1}
        
        mask = self._build_pandas_filter(filters, df=current_df)
        filtered_df = current_df[mask].copy()
        
        # æŸ¥æ‰¾é€‰ä¸­è¡Œçš„ä½ç½®
        matching_rows = filtered_df[filtered_df['id'] == row_id]
        if not matching_rows.empty:
            filtered_df_reset = filtered_df.reset_index(drop=True)
            matching_rows_reset = filtered_df_reset[filtered_df_reset['id'] == row_id]
            if not matching_rows_reset.empty:
                position = matching_rows_reset.index[0]
                return {
                    "found": True,
                    "position": int(position)
                }
        
        return {
            "found": False,
            "position": -1
        }
    
    def get_row_detail(self, row_id: Any) -> List[Dict[str, Any]]:
        """è·å–è¡Œçš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            row_id: è¡Œçš„IDå€¼
        
        Returns:
            è¡Œè¯¦æƒ…åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«labelã€valueã€detailã€typeç­‰å­—æ®µ
        """
        # ä½¿ç”¨é”ä¿æŠ¤è¯»å–
        with self._lock:
            current_df = self.dataframe
            if current_df is None:
                raise ValueError("DataFrame æœªåˆå§‹åŒ–")
        
        # åœ¨DataFrameä¸­é€šè¿‡IDåˆ—æŸ¥æ‰¾è¯¥è¡Œ
        matching_rows = current_df[current_df['id'] == row_id]
        if len(matching_rows) == 0:
            raise ValueError(f"æœªæ‰¾åˆ°IDä¸º {row_id} çš„è®°å½•")
        
        row_record = matching_rows.iloc[0].to_dict()
        
        # æ ¹æ®åˆ—é…ç½®ç”Ÿæˆè¯¦æƒ…
        detail = []
        for col_config in self.columns_config:
            prop = col_config.prop
            if prop in row_record:
                value = row_record[prop]
                # å¤„ç†ç‰¹æ®Šç±»å‹å­—æ®µçš„è½¬æ¢
                if isinstance(value, bytes):
                    # å¤„ç†bytesç±»å‹å­—æ®µï¼Œè½¬æ¢ä¸º16è¿›åˆ¶å­—ç¬¦ä¸²ç”¨äºJSONåºåˆ—åŒ–
                    value = ' '.join([f'{b:02X}' for b in value])
                elif prop == 'ts' and isinstance(value, (int, float)):
                    # å°†æ—¶é—´æˆ³ï¼ˆfloatï¼Œå¾®ç§’ç²¾åº¦ï¼‰è½¬æ¢ä¸ºæ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²
                    from datetime import datetime
                    try:
                        dt = datetime.fromtimestamp(float(value))
                        # æå–å¾®ç§’éƒ¨åˆ†ï¼ˆæ—¶é—´æˆ³çš„å°æ•°éƒ¨åˆ†è½¬æ¢ä¸ºå¾®ç§’ï¼‰
                        microseconds = int((float(value) % 1) * 1000000)
                        value = dt.strftime('%Y-%m-%d %H:%M:%S') + f'.{microseconds:06d}'
                    except (ValueError, OSError):
                        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸå€¼
                        pass
                
                detail_item = {
                    "label": col_config.label,
                    "value": value,
                    "detail": col_config.label,
                    "type": col_config.type
                }
                if col_config.type == 'number':
                    detail_item['format'] = 'int' if 'int' in str(current_df[prop].dtype) else 'float'
                detail.append(detail_item)
        
        return detail
    
    def update_dataframe(self, new_dataframe: pd.DataFrame) -> Dict[str, Any]:
        """ç›´æ¥æ›´æ–°DataFrame (ç”±å¤–éƒ¨æ§åˆ¶æ•°æ®æºæ—¶ä½¿ç”¨)
        
        Args:
            new_dataframe: æ–°çš„DataFrameæ•°æ®
        
        Returns:
            åŒ…å«æ›´æ–°ç»“æœçš„å­—å…¸
        """
        # ä½¿ç”¨é”ä¿æŠ¤å†™å…¥
        with self._lock:
            if new_dataframe is None:
                raise ValueError("DataFrameä¸èƒ½ä¸ºNone")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å­—æ®µ
            # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ columns_config å·²ç»åŒ…å«äº†ä¹‹å‰çš„æ‰€æœ‰å­—æ®µ
            existing_columns = set(c.prop for c in self.columns_config)
            new_columns = set(new_dataframe.columns)
            added_columns = new_columns - existing_columns
            
            columns_updated = False
            if added_columns:
                # ä¸ºæ–°å­—æ®µç”Ÿæˆåˆ—é…ç½®
                temp_df = new_dataframe[list(added_columns)]
                new_columns_config = generate_columns_config_from_dataframe(temp_df)
                self.columns_config.extend(new_columns_config)
                columns_updated = True
            
            # æ›´æ–° DataFrame å¼•ç”¨
            self.dataframe = new_dataframe
            
            # æ›´æ–°åˆ—é…ç½®ä¸­çš„ç­›é€‰é€‰é¡¹ï¼ˆå¯¹äº multi-select å’Œ select ç±»å‹ï¼‰
            for col_config in self.columns_config:
                if col_config.filterType in ['multi-select', 'select']:
                    try:
                        # æ£€æŸ¥è¯¥åˆ—æ˜¯å¦å­˜åœ¨äºæ–°æ•°æ®ä¸­
                        if col_config.prop not in self.dataframe.columns:
                            continue
                            
                        # æ€§èƒ½ä¼˜åŒ–ï¼šå…ˆæ£€æŸ¥å”¯ä¸€å€¼æ•°é‡
                        unique_count = self.dataframe[col_config.prop].nunique()
                        
                        if unique_count > 100:
                            if col_config.filterType != 'text':
                                col_config.options = None
                                col_config.filterType = 'text'
                                columns_updated = True
                        else:
                            unique_values = self.dataframe[col_config.prop].dropna().unique().tolist()
                            options = sorted([str(v) for v in unique_values])
                            
                            if col_config.options != options:
                                col_config.options = options
                                # options å˜åŒ–éœ€è¦é€šçŸ¥å‰ç«¯åˆ·æ–°åˆ—é…ç½®ï¼Œå¦åˆ™æ–°å‡ºç°çš„æšä¸¾å€¼å¯èƒ½æ— æ³•æ˜¾ç¤º
                                columns_updated = True
                    except Exception:
                        pass
            
            # éªŒè¯åˆ—é…ç½®
            self._validate_columns()
            
            return {
                "success": True,
                "columns_updated": columns_updated,
                "total_count": len(self.dataframe)
            }

    def add_data(self, new_data: Union[Dict[str, Any], List[Dict[str, Any]]]) -> Dict[str, Any]:
        """åŠ¨æ€æ·»åŠ æ–°æ•°æ®åˆ°DataFrame
        
        Args:
            new_data: æ–°æ•°æ®ï¼Œå¯ä»¥æ˜¯å•ä¸ªå­—å…¸æˆ–å­—å…¸åˆ—è¡¨
        
        Returns:
            åŒ…å«æ·»åŠ ç»“æœå’Œæ›´æ–°åçš„åˆ—é…ç½®çš„å­—å…¸
        """
        # ä½¿ç”¨é”ä¿æŠ¤å†™å…¥
        with self._lock:
            # æ£€æŸ¥ dataframe æ˜¯å¦æœ‰æ•ˆ
            if not hasattr(self, 'dataframe') or self.dataframe is None:
                raise ValueError("DataFrame æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ·»åŠ æ•°æ®")
            
            # ç¡®ä¿new_dataæ˜¯åˆ—è¡¨æ ¼å¼
            if isinstance(new_data, dict):
                new_data = [new_data]
            
            if not new_data:
                raise ValueError("æ–°æ•°æ®ä¸èƒ½ä¸ºç©º")
            
            # ä¿å­˜åŸå§‹æ•°æ®é‡ï¼Œç”¨äºéªŒè¯
            original_length = len(self.dataframe)
            original_columns = set(self.dataframe.columns)
            
            # è½¬æ¢ä¸ºDataFrame
            new_df = pd.DataFrame(new_data)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å­—æ®µï¼ˆä¸åœ¨ç°æœ‰DataFrameä¸­çš„å­—æ®µï¼‰
            existing_columns = set(self.dataframe.columns)
            new_columns = set(new_df.columns)
            added_columns = new_columns - existing_columns
            
            # å¦‚æœæ–°æ•°æ®ä¸­æœ‰æ–°å­—æ®µï¼Œéœ€è¦æ›´æ–°åˆ—é…ç½®
            columns_updated = False
            if added_columns:
                # ä¸ºæ–°å­—æ®µç”Ÿæˆåˆ—é…ç½®ï¼ˆgenerate_columns_config_from_dataframe å®šä¹‰åœ¨æœ¬æ–‡ä»¶æœ«å°¾ï¼‰
                # ç›´æ¥è°ƒç”¨ï¼Œæ— éœ€å¯¼å…¥
                
                # åˆ›å»ºä¸€ä¸ªä¸´æ—¶DataFrameï¼ŒåªåŒ…å«æ–°å­—æ®µï¼Œç”¨äºç”Ÿæˆåˆ—é…ç½®
                temp_df = new_df[list(added_columns)]
                new_columns_config = generate_columns_config_from_dataframe(temp_df)
                
                # å°†æ–°åˆ—é…ç½®æ·»åŠ åˆ°ç°æœ‰é…ç½®ä¸­
                self.columns_config.extend(new_columns_config)
                columns_updated = True
            
            # ç¡®ä¿æ–°æ•°æ®çš„åˆ—ä¸ç°æœ‰DataFrameçš„åˆ—å¯¹é½
            # å¯¹äºæ–°æ•°æ®ä¸­ä¸å­˜åœ¨çš„åˆ—ï¼Œå¡«å……None
            for col in existing_columns:
                if col not in new_df.columns:
                    new_df[col] = None
            
            # å¯¹äºç°æœ‰DataFrameä¸­ä¸å­˜åœ¨çš„åˆ—ï¼ˆæ–°å­—æ®µï¼‰ï¼Œåœ¨ç°æœ‰DataFrameä¸­å¡«å……None
            for col in added_columns:
                if col not in self.dataframe.columns:
                    self.dataframe[col] = None
            
            # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
            new_df = new_df[self.dataframe.columns]
            
            # å¤„ç†IDå­—æ®µï¼šå¦‚æœæ–°æ•°æ®æ²¡æœ‰IDæˆ–IDä¸ºNoneï¼Œè‡ªåŠ¨ç”Ÿæˆ
            if 'id' in self.dataframe.columns:
                max_id = self.dataframe['id'].max() if len(self.dataframe) > 0 else 0
                for idx, row in new_df.iterrows():
                    if pd.isna(row.get('id')) or row.get('id') is None:
                        max_id += 1
                        new_df.at[idx, 'id'] = max_id
            
            # å¤„ç†ç‰¹æ®Šç±»å‹å­—æ®µ
            for col in new_df.columns:
                col_config = next((c for c in self.columns_config if c.prop == col), None)
                if col_config:
                    if col_config.type == 'bytes':
                        # å¦‚æœå­—æ®µç±»å‹æ˜¯bytesï¼Œä½†æ–°æ•°æ®æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è½¬æ¢
                        for idx, val in new_df[col].items():
                            if isinstance(val, str):
                                # å°è¯•å°†16è¿›åˆ¶å­—ç¬¦ä¸²è½¬æ¢ä¸ºbytes
                                try:
                                    # ç§»é™¤ç©ºæ ¼å¹¶è½¬æ¢ä¸ºbytes
                                    hex_str = val.replace(' ', '').replace('-', '')
                                    new_df.at[idx, col] = bytes.fromhex(hex_str)
                                except ValueError:
                                    # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸå€¼
                                    pass
                    elif col == 'ts' and col_config.type == 'date':
                        # å¦‚æœtså­—æ®µæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è½¬æ¢ä¸ºæ—¶é—´æˆ³
                        from datetime import datetime
                        for idx, val in new_df[col].items():
                            if pd.isna(val) or val is None:
                                continue
                            if isinstance(val, str) and val.strip():
                                try:
                                    # å°è¯•è§£ææ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
                                    val_stripped = val.strip()
                                    # å°è¯•å®Œæ•´æ ¼å¼ï¼šYYYY-MM-DD HH:MM:SS.ffffff
                                    try:
                                        dt = datetime.strptime(val_stripped, '%Y-%m-%d %H:%M:%S.%f')
                                        new_df.at[idx, col] = dt.timestamp()
                                    except ValueError:
                                        # å°è¯•æ ¼å¼ï¼šYYYY-MM-DD HH:MM:SS
                                        try:
                                            dt = datetime.strptime(val_stripped, '%Y-%m-%d %H:%M:%S')
                                            new_df.at[idx, col] = dt.timestamp()
                                        except ValueError:
                                            # å°è¯•æ ¼å¼ï¼šYYYY-MM-DD
                                            try:
                                                dt = datetime.strptime(val_stripped, '%Y-%m-%d')
                                                new_df.at[idx, col] = dt.timestamp()
                                            except ValueError:
                                                # å¦‚æœéƒ½å¤±è´¥ï¼Œå°è¯•ä½œä¸ºæ•°å­—ï¼ˆå¯èƒ½æ˜¯æ—¶é—´æˆ³å­—ç¬¦ä¸²ï¼‰
                                                try:
                                                    new_df.at[idx, col] = float(val_stripped)
                                                except ValueError:
                                                    pass
                                except Exception:
                                    pass
            
            # å°†æ–°æ•°æ®è¿½åŠ åˆ°DataFrame
            # ä½¿ç”¨ ignore_index=True ç¡®ä¿ç´¢å¼•è¿ç»­ï¼Œé¿å…ç´¢å¼•é—®é¢˜
            # æ³¨æ„ï¼šignore_index=True å·²ç»ä¼šé‡ç½®ç´¢å¼•ï¼Œä¸éœ€è¦å†è°ƒç”¨ reset_index
            try:
                # æ‰§è¡Œåˆå¹¶æ“ä½œ
                combined_df = pd.concat([self.dataframe, new_df], ignore_index=True)
                
                # éªŒè¯åˆå¹¶åçš„æ•°æ®é‡æ˜¯å¦æ­£ç¡®
                expected_length = original_length + len(new_df)
                if len(combined_df) != expected_length:
                    raise ValueError(
                        f"æ•°æ®åˆå¹¶åé•¿åº¦ä¸åŒ¹é…: æœŸæœ› {expected_length}, å®é™… {len(combined_df)}. "
                        f"åŸå§‹æ•°æ®é‡: {original_length}, æ–°æ•°æ®é‡: {len(new_df)}"
                    )
                
                # éªŒè¯åˆ—æ˜¯å¦ä¸€è‡´
                if set(combined_df.columns) != original_columns:
                    missing_columns = original_columns - set(combined_df.columns)
                    if missing_columns:
                        raise ValueError(f"åˆå¹¶åç¼ºå°‘åˆ—: {missing_columns}")
                
                # éªŒè¯æ•°æ®æ²¡æœ‰è¢«æ¸…ç©ºï¼ˆåˆå¹¶åçš„æ•°æ®é‡åº”è¯¥å¤§äºç­‰äºåŸå§‹æ•°æ®é‡ï¼‰
                if len(combined_df) < original_length:
                    raise ValueError(
                        f"æ•°æ®åˆå¹¶åæ•°æ®é‡å‡å°‘: åŸå§‹ {original_length}, åˆå¹¶å {len(combined_df)}. "
                        f"è¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œå¯èƒ½æ˜¯æ•°æ®è¢«æ„å¤–æ¸…ç©º"
                    )
                
                # æ‰€æœ‰éªŒè¯é€šè¿‡åï¼Œæ‰æ›´æ–° self.dataframe
                self.dataframe = combined_df
                
                # è®°å½•æ·»åŠ æ•°æ®çš„ä¿¡æ¯
                import logging
                logger = logging.getLogger(__name__)
                logger.info(
                    f"add_data æˆåŠŸ: æ·»åŠ äº† {len(new_df)} è¡Œ, "
                    f"åŸå§‹æ•°æ®é‡={original_length}, æ–°æ•°æ®é‡={len(self.dataframe)}"
                )
                
            except Exception as e:
                import logging
                logger = logging.getLogger(__name__)
                current_length = len(self.dataframe) if hasattr(self, 'dataframe') and self.dataframe is not None else 'N/A'
                logger.error(
                    f"æ·»åŠ æ•°æ®å¤±è´¥: {str(e)}, å½“å‰æ•°æ®é‡={current_length}, "
                    f"æ–°æ•°æ®é‡={len(new_df)}, åŸå§‹æ•°æ®é‡={original_length}",
                    exc_info=True
                )
                # ç¡®ä¿åœ¨å¼‚å¸¸æƒ…å†µä¸‹ï¼Œdataframe æ²¡æœ‰è¢«ç ´å
                if not hasattr(self, 'dataframe') or self.dataframe is None or len(self.dataframe) < original_length:
                    logger.critical(
                        f"æ£€æµ‹åˆ° DataFrame å¯èƒ½è¢«ç ´åï¼åŸå§‹æ•°æ®é‡: {original_length}, "
                        f"å½“å‰æ•°æ®é‡: {current_length}. è¿™å¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±ï¼"
                    )
                raise
            
            # æ›´æ–°åˆ—é…ç½®ä¸­çš„ç­›é€‰é€‰é¡¹ï¼ˆå¯¹äº multi-select å’Œ select ç±»å‹ï¼‰
            # é‡æ–°è®¡ç®—å”¯ä¸€å€¼å¹¶æ›´æ–° options
            for col_config in self.columns_config:
                if col_config.filterType in ['multi-select', 'select']:
                    try:
                        # æ€§èƒ½ä¼˜åŒ–ï¼šå…ˆæ£€æŸ¥å”¯ä¸€å€¼æ•°é‡ï¼Œå¦‚æœå¤ªå¤šç›´æ¥åˆ‡æ¢ä¸ºæ–‡æœ¬ç­›é€‰ï¼Œé¿å…è®¡ç®— unique()
                        # unique() åœ¨æ•°æ®é‡å¤§æ—¶æ¯”è¾ƒè€—æ—¶
                        unique_count = self.dataframe[col_config.prop].nunique()
                        
                        if unique_count > 100:
                            # å¦‚æœè¶…è¿‡100ä¸ªï¼Œæ¸…ç©º optionsï¼Œä½¿ç”¨æ–‡æœ¬ç­›é€‰
                            if col_config.filterType != 'text':
                                col_config.options = None
                                col_config.filterType = 'text'
                                columns_updated = True
                        else:
                            # è·å–è¯¥åˆ—çš„å”¯ä¸€å€¼ï¼ˆæ•°é‡ä¸å¤šï¼Œè®¡ç®—å¼€é”€å¯æ¥å—ï¼‰
                            unique_values = self.dataframe[col_config.prop].dropna().unique().tolist()
                            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ’åº
                            options = sorted([str(v) for v in unique_values])
                            
                            # æ£€æŸ¥ options æ˜¯å¦æœ‰å˜åŒ–
                            if col_config.options != options:
                                col_config.options = options
                                # options å˜åŒ–éœ€è¦é€šçŸ¥å‰ç«¯åˆ·æ–°åˆ—é…ç½®ï¼Œå¦åˆ™æ–°å‡ºç°çš„æšä¸¾å€¼å¯èƒ½æ— æ³•æ˜¾ç¤º
                                columns_updated = True
                                
                    except Exception as e:
                        # å¦‚æœæ›´æ–°å¤±è´¥ï¼Œä¿æŒåŸæœ‰é…ç½®
                        pass
            
            # éªŒè¯åˆ—é…ç½®
            self._validate_columns()
            
            return {
                "success": True,
                "added_count": len(new_df),
                "columns_updated": columns_updated,
                "added_columns": list(added_columns) if added_columns else []
            }


def generate_columns_config_from_dataframe(df: pd.DataFrame) -> List[ColumnConfig]:
    """æ ¹æ®DataFrameè‡ªåŠ¨ç”Ÿæˆåˆ—é…ç½®
    
    Args:
        df: pandas DataFrame
    
    Returns:
        åˆ—é…ç½®åˆ—è¡¨
    """
    columns_config = []
    
    for col in df.columns:
        col_type = str(df[col].dtype)
        column_type = 'string'
        filter_type = 'text'
        sortable = True
        filterable = True
        min_width = 120
        fixed = False
        options = None
        
        # æ ¹æ®æ•°æ®ç±»å‹è®¾ç½®ç±»å‹å’Œç­›é€‰æ–¹å¼
        if col.lower() == 'ts':
            # tså­—æ®µä¼˜å…ˆè¯†åˆ«ä¸ºæ—¥æœŸç±»å‹
            column_type = 'date'
            filter_type = 'date'
        elif 'int' in col_type or 'float' in col_type:
            column_type = 'number'
            filter_type = 'number'
        elif 'datetime' in col_type or 'date' in col.lower():
            # è‡ªåŠ¨è¯†åˆ«æ—¥æœŸå­—æ®µï¼ˆdatetimeç±»å‹ã€å­—æ®µååŒ…å«dateæˆ–tså­—æ®µï¼‰
            column_type = 'date'
            filter_type = 'date'
        elif col == 'id':
            # IDå­—æ®µå›ºå®šå·¦ä¾§ï¼Œä½¿ç”¨æ•°å­—ç­›é€‰
            filter_type = 'number'
            fixed = 'left'
        elif 'bytes' in col.lower() or 'hex' in col.lower() or 'remark' in col.lower() or 'payload' in col.lower():
            # è¯†åˆ«bytesç±»å‹å­—æ®µï¼ˆé€šè¿‡å­—æ®µåè¯†åˆ«ï¼‰
            column_type = 'bytes'
            filter_type = 'text'  # bytesç±»å‹ä½¿ç”¨æ–‡æœ¬ç­›é€‰
            min_width = 200  # bytesç±»å‹å­—æ®µé€šå¸¸éœ€è¦æ›´å®½çš„æ˜¾ç¤ºç©ºé—´
        elif 'object' in col_type:
            # æ£€æŸ¥æ˜¯å¦ä¸ºçœŸæ­£çš„bytesç±»å‹
            sample_value = df[col].dropna().iloc[0] if not df[col].dropna().empty else None
            if isinstance(sample_value, bytes):
                column_type = 'bytes'
                filter_type = 'text'
                min_width = 200
            else:
                # ä¸æ˜¯bytesç±»å‹ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œç„¶åæ£€æŸ¥å”¯ä¸€å€¼æ•°é‡
                sample_values = df[col].dropna().head(10).tolist()
                is_hex_string = False
                if sample_values:
                    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ ·æœ¬å€¼éƒ½æ˜¯16è¿›åˆ¶å­—ç¬¦ä¸²æ ¼å¼ï¼ˆå¦‚ "FF 00 1A" æˆ– "FF001A"ï¼‰
                    # ä¼˜åŒ–ï¼šå¢åŠ é•¿åº¦é™åˆ¶ï¼Œé¿å…å°†æ™®é€šé•¿å­—ç¬¦ä¸²è¯¯åˆ¤ä¸º hex
                    hex_pattern = re.compile(r'^([0-9A-Fa-f]{2}[\s]*)+$')
                    is_hex_string = all(
                        isinstance(v, str) and (hex_pattern.match(v.replace(' ', '')) or (len(v) > 20 and len(v) % 2 == 0 and all(c in '0123456789abcdefABCDEF ' for c in v)))
                        for v in sample_values if v
                    )
                    # å¦‚æœå­—æ®µååŒ…å«ç›¸å…³å…³é”®è¯ï¼Œä¹Ÿè®¤ä¸ºæ˜¯bytesç±»å‹
                    if is_hex_string or any(keyword in col.lower() for keyword in ['bytes', 'hex', 'binary', 'data', 'payload']):
                        column_type = 'bytes'
                        filter_type = 'text'
                        min_width = 200
                    else:
                        # å¯å‘å¼è§„åˆ™ï¼šå¦‚æœæ˜¯ IDã€ç¼–å·ã€Code ç­‰å­—æ®µï¼Œé€šå¸¸æ˜¯é«˜åŸºæ•°çš„ï¼Œç›´æ¥ä½¿ç”¨æ–‡æœ¬ç­›é€‰
                        # é¿å…ä¸€å¼€å§‹è¯¯åˆ¤ä¸º multi-select
                        is_id_like = any(keyword in col.lower() for keyword in ['id', 'no', 'number', 'code', 'uuid', 'guid'])
                        
                        if is_id_like:
                            filter_type = 'text'
                        else:
                            unique_values = df[col].unique().tolist()
                            if len(unique_values) <= 100:  # å¦‚æœå”¯ä¸€å€¼å°‘äº100ä¸ªï¼Œæä¾›ä¸‹æ‹‰é€‰é¡¹
                                filter_type = 'multi-select'
                                options = [str(v) for v in unique_values]
                            else:
                                # å”¯ä¸€å€¼å¤ªå¤šï¼Œä½¿ç”¨æ–‡æœ¬ç­›é€‰
                                filter_type = 'text'
                else:
                    filter_type = 'text'
        else:
            # å¯¹äºå­—ç¬¦ä¸²ç±»å‹ï¼Œæ£€æŸ¥å”¯ä¸€å€¼æ•°é‡
            if 'string' in col_type:
                # æ£€æŸ¥æ•°æ®å†…å®¹æ˜¯å¦çœ‹èµ·æ¥åƒ16è¿›åˆ¶å­—ç¬¦ä¸²ï¼ˆbytesçš„å¸¸è§è¡¨ç¤ºå½¢å¼ï¼‰
                sample_values = df[col].dropna().head(10).tolist()
                is_hex_string = False
                if sample_values:
                    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ ·æœ¬å€¼éƒ½æ˜¯16è¿›åˆ¶å­—ç¬¦ä¸²æ ¼å¼ï¼ˆå¦‚ "FF 00 1A" æˆ– "FF001A"ï¼‰
                    # ä¼˜åŒ–ï¼šå¢åŠ é•¿åº¦é™åˆ¶ï¼Œé¿å…å°†æ™®é€šé•¿å­—ç¬¦ä¸²è¯¯åˆ¤ä¸º hex
                    hex_pattern = re.compile(r'^([0-9A-Fa-f]{2}[\s]*)+$')
                    is_hex_string = all(
                        isinstance(v, str) and (hex_pattern.match(v.replace(' ', '')) or (len(v) > 20 and len(v) % 2 == 0 and all(c in '0123456789abcdefABCDEF ' for c in v)))
                        for v in sample_values if v
                    )
                    # å¦‚æœå­—æ®µååŒ…å«ç›¸å…³å…³é”®è¯ï¼Œä¹Ÿè®¤ä¸ºæ˜¯bytesç±»å‹
                    if is_hex_string or any(keyword in col.lower() for keyword in ['bytes', 'hex', 'binary', 'data', 'payload']):
                        column_type = 'bytes'
                        filter_type = 'text'
                        min_width = 200
                    else:
                        # å¯å‘å¼è§„åˆ™ï¼šå¦‚æœæ˜¯ IDã€ç¼–å·ã€Code ç­‰å­—æ®µï¼Œé€šå¸¸æ˜¯é«˜åŸºæ•°çš„ï¼Œç›´æ¥ä½¿ç”¨æ–‡æœ¬ç­›é€‰
                        is_id_like = any(keyword in col.lower() for keyword in ['id', 'no', 'number', 'code', 'uuid', 'guid'])
                        
                        if is_id_like:
                            filter_type = 'text'
                        else:
                            unique_values = df[col].unique().tolist()
                            if len(unique_values) <= 100:  # å¦‚æœå”¯ä¸€å€¼å°‘äº100ä¸ªï¼Œæä¾›ä¸‹æ‹‰é€‰é¡¹
                                filter_type = 'multi-select'
                                options = [str(v) for v in unique_values]
                            else:
                                # å”¯ä¸€å€¼å¤ªå¤šï¼Œä½¿ç”¨æ–‡æœ¬ç­›é€‰
                                filter_type = 'text'
                else:
                    filter_type = 'text'
            else:
                filter_type = 'text'
        
        columns_config.append(ColumnConfig(
            prop=col,
            label=col,
            type=column_type,
            sortable=sortable,
            filterable=filterable,
            filterType=filter_type,
            minWidth=min_width,
            fixed=fixed,
            options=options
        ))
    
    return columns_config

